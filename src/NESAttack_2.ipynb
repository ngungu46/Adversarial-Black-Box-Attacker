{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 22:42:17.466244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 22:42:17.466300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 22:42:17.467716: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 22:42:17.476572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 22:42:18.635595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "# import GPyOpt as gy\n",
    "# import noise as ns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import keras\n",
    "\n",
    "# import utility and helper functions/classes \n",
    "from src.model import *\n",
    "from src.dataloader import * \n",
    "from src.utils import * \n",
    "\n",
    "\n",
    "import time as time\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(dataset, device): \n",
    "    if dataset == \"imagenet64\": \n",
    "        pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device) \n",
    "        pretrained_model.eval()\n",
    "        return pretrained_model \n",
    "    \n",
    "    elif dataset == \"butterflies_and_moths\": \n",
    "        pretrained_model = keras.models.load_model(os.path.join(\"HSJattack\", \"EfficientNetB0_butterfly.h5\"), custom_objects={'F1_score':'F1_score'}) \n",
    "        pretrained_model.trainable = False \n",
    "        return pretrained_model \n",
    "    \n",
    "    else: \n",
    "        raise Exception(\"Not a valid dataset name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /hpc/home/mb625/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/scratch/mb625/anaconda3/envs/DL_final/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/mb625/anaconda3/envs/DL_final/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "torch_model = getModel(\"imagenet64\", \"cuda:1\")\n",
    "flow_model = getModel(\"butterflies_and_moths\", \"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random subset of \"dataset_size\" validation dataset image filepaths and store them in a list \n",
    "image_filepaths = getSubset(\"imagenet64\", 100, True)\n",
    "\n",
    "# get class2index and index2 class dictionaries for easy querying \n",
    "image_cls2idx = class2index(\"imagenet64\") \n",
    "image_idx2cls = {v: k for k, v in image_cls2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "torch.Size([1, 100])\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test out for pytorch \n",
    "for i, imgpath in enumerate(image_filepaths): \n",
    "    rawimage = Image.open(imgpath)\n",
    "    rawimage = tf.keras.preprocessing.image.img_to_array(rawimage) / 255\n",
    "    rawimage = cv2.resize(rawimage, dsize=(224, 224))\n",
    "    \n",
    "    # image = torch.tensor(rawimage, dtype=torch.float32).to(\"cuda\")\n",
    "    # image = torch.unsqueeze(image, 0)\n",
    "    # print(image.shape)\n",
    "    # prediction = predict(image, torch_model, is_torch=True)\n",
    "\n",
    "    image = tf.cast(rawimage, tf.float32)\n",
    "    image = image[None, ...]\n",
    "    print(image.shape, type(image))\n",
    "    prediction = flow_model.predict(image * 255, steps=1) \n",
    "    prediction = torch.tensor(prediction) \n",
    "    print(prediction.shape)\n",
    "    print(predict(image, flow_model, is_torch=False).shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(prediction.shape) \n",
    "    print(type(prediction))        \n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(image, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# pred = predict(image, torch_model, is_torch=True, device=\"cuda\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m flow_model\u001b[39m.\u001b[39;49mpredict(image)\n",
      "File \u001b[0;32m/scratch/mb625/anaconda3/envs/DL_final/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/scratch/mb625/anaconda3/envs/DL_final/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:852\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(type_value, _dtypes\u001b[39m.\u001b[39mDType):\n\u001b[1;32m    850\u001b[0m   \u001b[39mreturn\u001b[39;00m _INTERN_TABLE[type_value\u001b[39m.\u001b[39mas_datatype_enum]\n\u001b[0;32m--> 852\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot convert the argument `type_value`: \u001b[39m\u001b[39m{\u001b[39;00mtype_value\u001b[39m!r}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mto a TensorFlow DType.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType."
     ]
    }
   ],
   "source": [
    "for i, imgpath in enumerate(image_filepaths): \n",
    "    rawimage = np.array(Image.open(imgpath)) \n",
    "    rawimage = cv2.resize(rawimage, dsize=(299, 299))\n",
    "    \n",
    "    image = torch.tensor(rawimage, dtype=torch.float32).to(\"cuda\")\n",
    "    if image.size() == (299, 299): \n",
    "        # if image is grayscale, then repeat the dimensions \n",
    "        image = image.unsqueeze(-1) \n",
    "        image = image.repeat(1, 1, 3)\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    \n",
    "    # pred = predict(image, torch_model, is_torch=True, device=\"cuda\")\n",
    "    flow_model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 1000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m rawimage \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(imgpath)) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m rawimage \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(rawimage, dsize\u001b[39m=\u001b[39m(\u001b[39m299\u001b[39m, \u001b[39m299\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(rawimage, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m (\u001b[39m299\u001b[39m, \u001b[39m299\u001b[39m): \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# if image is grayscale, then repeat the dimensions \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btarokhlab-02/scratch/mb625/Adversarial-Black-Box-Attacker/NESAttack_2.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, imgpath in enumerate(image_filepaths): \n",
    "    rawimage = np.array(Image.open(imgpath)) / 255\n",
    "    rawimage = cv2.resize(rawimage, dsize=(299, 299))\n",
    "    \n",
    "    image = torch.tensor(rawimage, dtype=torch.float32).to(\"cuda\")\n",
    "    if image.size() == (299, 299): \n",
    "        # if image is grayscale, then repeat the dimensions \n",
    "        image = image.unsqueeze(-1) \n",
    "        image = image.repeat(1, 1, 3)\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    \n",
    "    pred = predict(image, torch_model, is_torch=True, device=\"cuda\")\n",
    "    \n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, model, is_torch=True, device='cuda'):\n",
    "    \"\"\"\n",
    "    input: normalized tensor of shape (B, C, H, W)\n",
    "    output: numpy array of predictions\n",
    "    \"\"\"\n",
    "    # print(\"predicting\")\n",
    "    if is_torch: \n",
    "        with torch.no_grad():\n",
    "            preds = model(transforming(image).to(device))\n",
    "        return preds\n",
    "    else: \n",
    "        pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
